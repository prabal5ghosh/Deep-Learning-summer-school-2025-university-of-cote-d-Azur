{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabal5ghosh/Deep-Learning-summer-school-2025-university-of-cote-d-Azur/blob/main/TPtokenizer_SUJET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prabal Ghosh  "
      ],
      "metadata": {
        "id": "bV3SNGi6GLDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# date- 23/06/2025"
      ],
      "metadata": {
        "id": "gsvZBkFNGOt4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HDBlMabe7gp"
      },
      "source": [
        "<center><img width=60% src=\"http://www.i3s.unice.fr/~lingrand/efeliaUnica.png\"><br/><br/>\n",
        "<font size=+3><b>TP tokenization</b></font><br/><br/>\n",
        "    <font size=+1>Diane Lingrand, CÃ©lia D'cruz and FrÃ©dÃ©ric Precioso<br/><br/>\n",
        "    2025 - June/July</font><br/>\n",
        "    <img width=14% src=\"http://www.i3s.unice.fr/~lingrand/cc-long.png\">\n",
        "    </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVVnZpEDr5k"
      },
      "source": [
        "In this notebook, you will experiment one very simple tokenizer and the tokenizer that is part of GPT family: BPE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7Sbx4GLDr5l"
      },
      "source": [
        "The idea is to rewrite these algorithms in order to deeply understand their principles (and limitations) before using more effective implementation using librairies such as `tiktoken` or from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tRU8gT57Dr5j"
      },
      "outputs": [],
      "source": [
        "# necessary imports\n",
        "import re # for regular expressions\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_xitTtfDr5m"
      },
      "source": [
        "# A vanilla tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwLZ769Dr5m"
      },
      "source": [
        "In this section, we will build a very simple tokenizer that aims at experimenting basics ideas and problems. Usually, tokenizers are composed of two parts:\n",
        "- the pre-tokenizer that splits a text into tokens (letters, words ...)\n",
        "- a tokenizer that performs different operations such as merging, splitting, counting occurrences... and assigns a numerical code to each final token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SlQ9j2uDr5n"
      },
      "source": [
        "Our vanilla tokenizer will split a text into words, build a dictionary and encode each token by its index in the dictionary.\n",
        "\n",
        "\n",
        "Example: \"Welcome to the 2025 Deep Learning School! Deep Learning is fun. Hope you will learn a lot at the school!\" lead to the dictionary containing:\n",
        "\n",
        "<table><tr><td>welcome</td><td>to</td><td>the</td><td>2025</td><td>deep</td><td>learning</td><td>school</td><td>is</td><td>fun</td><td>hope</td><td>you</td><td>will</td><td>learn</td><td>a</td><td>lot</td><td>at</td><td>!</td></tr>\n",
        "    <tr><td>0 </td><td>1 </td><td>2 </td><td>3 </td><td>4 </td><td>5 </td><td>6</td><td> 7 </td><td>8 </td><td>9 </td><td>10 </td><td>11</td><td> 12</td><td> 13</td><td> 14</td><td> 15 </td><td>16</td><td>17</td></tr></table>\n",
        "\n",
        "Thus, the encoding of the previous sentence becomes:\n",
        "\n",
        "<pre>0 1 2 3 4 5 6 16 4 5 7 8 17 9 10 11 12 13 14 15 2 6 16</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRlteBooDr5n"
      },
      "source": [
        "## Splitting a text into words and punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rAkS20ce7gw"
      },
      "source": [
        "For this part, you will use regular expression using the [`re`](https://docs.python.org/3/library/re.html) library. As you may have noticed, we remove the space character in order to save space. Be aware that in some applications, it could be necessary to add the space character to the dictionary and to encode it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wksE0sjNDr5o"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Using the [`lower` function](https://docs.python.org/3.3/library/stdtypes.html#str.lower), convert the string below into lower case letters. Then, using the [`split` function](https://docs.python.org/3/library/re.html#re.split), split the resulting text into words AND punctuations. For example, \"Deep Learning School!!!\" should be converted to [\"deep\", \"learning\", \"school\", \"!\", \"!\", \"!\"]. Start by lowercasing the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCkETzecDr5q"
      },
      "outputs": [],
      "source": [
        "text = \"Welcome to the 2025 Deep Learning School! Deep Learning is fun. Hope you will learn a lot at the school!\" # this text will be reused multiple times in the beginning of the lab\n",
        "\n",
        "# your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "wOZiV2KzDr5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef6cd7c-079d-4b8d-86f5-8b180c25eb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome to the 2025 deep learning school! deep learning is fun. hope you will learn a lot at the school!\n"
          ]
        }
      ],
      "source": [
        "# hint 1: lower letters\n",
        "textLower = text.lower()\n",
        "print(textLower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "myGHUggQDr5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31df2457-37ac-46da-c814-d4e7e9548aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', ' ', 'to', ' ', 'the', ' ', '2025', ' ', 'deep', ' ', 'learning', ' ', 'school!', ' ', 'deep', ' ', 'learning', ' ', 'is', ' ', 'fun.', ' ', 'hope', ' ', 'you', ' ', 'will', ' ', 'learn', ' ', 'a', ' ', 'lot', ' ', 'at', ' ', 'the', ' ', 'school!']\n",
            "\n",
            "\n",
            "['welcome', 'to', 'the', '2025', 'deep', 'learning', 'school', '', 'deep', 'learning', 'is', 'fun', '', 'hope', 'you', 'will', 'learn', 'a', 'lot', 'at', 'the', 'school', '']\n",
            "\n",
            "\n",
            "['welcome to the 2025 deep learning school', ' deep learning is fun', ' hope you will learn a lot at the school', '']\n"
          ]
        }
      ],
      "source": [
        "# hint 2: re.split usage\n",
        "# in this example, the splitting is done for every space (and only spaces)\n",
        "textSplitted = re.split(r'(\\s)',textLower)\n",
        "print(textSplitted)\n",
        "print()\n",
        "print()\n",
        "# splitting with everything not a letter or a digit (space, punctuation ...)\n",
        "textSplitted2 = re.split(r'[^a-z0-9]',textLower)\n",
        "print(textSplitted2)\n",
        "print()\n",
        "print()\n",
        "# splitting using punctuations only\n",
        "textSplitted3 = re.split(r'[,.!;:?]', textLower)\n",
        "print(textSplitted3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZryW1QzDr5t"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Compute the list of unique tokens (lowercased words and punctuations) of the text from the beginning of the lab using the [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "lGdJx3nzIzYF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "code_folding": [],
        "id": "Ehmx0HybDr5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bc5a7b-0a3b-4e22-a148-3dc0dac69e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['' '2025' 'a' 'at' 'deep' 'fun' 'hope' 'is' 'learn' 'learning' 'lot'\n",
            " 'school' 'the' 'to' 'welcome' 'will' 'you']\n"
          ]
        }
      ],
      "source": [
        "# your work\n",
        "unique_tokens = np.unique(textSplitted2)\n",
        "print(unique_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your work\n",
        "unique_tokens = np.unique(textSplitted2)\n",
        "print(unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmv1jshgI81i",
        "outputId": "f13af00e-9456-4d47-abfe-69e27eed9069"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['' '2025' 'a' 'at' 'deep' 'fun' 'hope' 'is' 'learn' 'learning' 'lot'\n",
            " 'school' 'the' 'to' 'welcome' 'will' 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLgwzbXDr5v"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Using the list of unique tokens, create the dictionary `token2id` that associates each unique token (dictionary key) to an unique ID (dictionary value): the beginning of the lab provides an example of the dictionary. Then, create the `id2token` dictionary that peforms the inverse associations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WXHoxku3Dr5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150ec2b7-0ae4-422a-ffc9-e1135380a291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token2id: {np.str_(''): 0, np.str_('2025'): 1, np.str_('a'): 2, np.str_('at'): 3, np.str_('deep'): 4, np.str_('fun'): 5, np.str_('hope'): 6, np.str_('is'): 7, np.str_('learn'): 8, np.str_('learning'): 9, np.str_('lot'): 10, np.str_('school'): 11, np.str_('the'): 12, np.str_('to'): 13, np.str_('welcome'): 14, np.str_('will'): 15, np.str_('you'): 16}\n",
            "\n",
            "id2token: {0: np.str_(''), 1: np.str_('2025'), 2: np.str_('a'), 3: np.str_('at'), 4: np.str_('deep'), 5: np.str_('fun'), 6: np.str_('hope'), 7: np.str_('is'), 8: np.str_('learn'), 9: np.str_('learning'), 10: np.str_('lot'), 11: np.str_('school'), 12: np.str_('the'), 13: np.str_('to'), 14: np.str_('welcome'), 15: np.str_('will'), 16: np.str_('you')}\n"
          ]
        }
      ],
      "source": [
        "# your work\n",
        "token2id = {token: i for i, token in enumerate(unique_tokens)}\n",
        "id2token = {i: token for i, token in enumerate(unique_tokens)}\n",
        "\n",
        "print(\"token2id:\", token2id)\n",
        "print(\"\\nid2token:\", id2token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkz4lPDnDr5w"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Using the previous dictionaries, encode the text from the beginning of the lab. Display the encoded text. It should be a list of IDs, where the Nth ID in this list represents the Nth word or punctuation in the initial text. The beginning of the lab provides an example of the encoded text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4ETh6aehLFAa",
        "outputId": "d51af11d-5a61-448f-9189-fce203016d13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Welcome to the 2025 Deep Learning School! Deep Learning is fun. Hope you will learn a lot at the school!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8TamOlWkDr5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30da39b5-2723-4e9c-ba17-d74d80ceef98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14, 13, 12, 1, 4, 9, 11, 0, 4, 9, 7, 5, 0, 6, 16, 15, 8, 2, 10, 3, 12, 11, 0]\n"
          ]
        }
      ],
      "source": [
        "# your work\n",
        "encoded_text = [token2id[token] for token in textSplitted2]\n",
        "print(encoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHbnOaTwDr5y"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Using the previous dictionaries, perform a simple decoding of the encoded text (list of IDs) from the previous question, as an first rough attempt to reconstruct the initial text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7FDJKpXWDr5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a189376f-e1c4-4bf3-f90a-f0fb9ccba1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome to the 2025 deep learning school  deep learning is fun  hope you will learn a lot at the school \n"
          ]
        }
      ],
      "source": [
        "# your work\n",
        "decoded_text = ' '.join([id2token[id] for id in encoded_text])\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your work\n",
        "decoded_text = ''\n",
        "for id in encoded_text:\n",
        "    decoded_text += ' ' + id2token[id]\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqWMrZc9Loow",
        "outputId": "bf63f9ae-7548-47d5-e2f8-c983dd2cfa94"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " welcome to the 2025 deep learning school  deep learning is fun  hope you will learn a lot at the school \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e6t-ZgfnMERj",
        "outputId": "e7174d11-dcee-4633-82e6-58da366005e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Welcome to the 2025 Deep Learning School! Deep Learning is fun. Hope you will learn a lot at the school!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_d8k4x3Dr5z"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Are you satisfied with the decoding? What information was lost during the process of pretokenizing and tokenizing the initial text?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No the decoding is not able to generate the initial text properly. the decoded text does not contain any ponctuation.  example ! .\n",
        "\n",
        "as duing tokenization we remove the punctuation."
      ],
      "metadata": {
        "id": "oZuyAGbpMP6X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "code_folding": [
          0,
          3
        ],
        "id": "vjPBOUvUDr50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590c1b27-9a26-478f-9163-6799e8d7f24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Welcome to the 2025 Deep Learning School!  Deep Learning is fun.  Hope you will learn a lot at the school! \n",
            "\n",
            "\n",
            "Welcome to the 2025 Deep Learning School! Deep Learning is fun. Hope you will learn a lot at the school!\n"
          ]
        }
      ],
      "source": [
        "# your answer\n",
        "# ...\n",
        "\n",
        "# your code to perform another attempt that is more complex to better decode the text: use common English rules to make up for the loss of information\n",
        "# ...\n",
        "\n",
        "# textLower = text.lower()\n",
        "textLower = text\n",
        "\n",
        "# print(textLower)\n",
        "res = re.findall( r'\\w+|[^\\s\\w]+', textLower)\n",
        "# print(res)\n",
        "\n",
        "# your work\n",
        "unique_tokens = np.unique(res)\n",
        "# print(unique_tokens)\n",
        "\n",
        "# your work\n",
        "token2id = {token: i for i, token in enumerate(unique_tokens)}\n",
        "id2token = {i: token for i, token in enumerate(unique_tokens)}\n",
        "\n",
        "# print(\"token2id:\", token2id)\n",
        "# print(\"\\nid2token:\", id2token)\n",
        "\n",
        "# your work\n",
        "encoded_text = [token2id[token] for token in res]\n",
        "# print(encoded_text)\n",
        "\n",
        "decoded_text = ''\n",
        "for id in encoded_text:\n",
        "  if id2token[id] == \".\" or id2token[id] == \"!\":\n",
        "      decoded_text += id2token[id]+ ' '\n",
        "  else:\n",
        "      decoded_text += ' ' + id2token[id]\n",
        "\n",
        "print(decoded_text)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "TOxdTa1fDr51"
      },
      "source": [
        "## Writing a python class for encoding / decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "x0EghfF3Dr52"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Organize all the previous pieces of code into a python object representing this tokenizer. Write a constructor that will build the dictionnaries from a corpus (list of strings). Then, write an encode and a decode methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "kLA7P9HhDr52"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "hidden": true,
        "id": "wsaLj3agDr53"
      },
      "outputs": [],
      "source": [
        "# hint: the skeleton\n",
        "class VanillaTokenizer:\n",
        "\n",
        "    def __init__(self, corpus):\n",
        "        # corpus is a list of texts. Concatenate the texts. It will be used to create the dictionaries.\n",
        "        # Do all the necessary steps to create the dictionaries\n",
        "        self.token2id =\n",
        "        self.id2token =\n",
        "\n",
        "    def encode(self, text):\n",
        "        # We want to create an ordered list of IDs that corresponds to the encoded text (Nth ID in this list represents the Nth word or punctuation in the text)\n",
        "        # Do all the necessary steps to create this list, and return it\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # the \"ids\" parameter corresponds to the encoded text (ordered list of IDs). We want to decoded it to get the initial text.\n",
        "        # Do all the necessary steps to create this decoded text, and return it\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wHznAyamDr54"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Using the class VanillaTokenizer, build a tokenizer object initialized from the text at the beginning of the lab: \"Welcome to the 2025 Deep Learning School! Deep Learning is fun. Hope you will learn a lot at the school!\". Then, try to encode the 2 following texts: \"Deep Learning School is fun!\" and \"Welcome to Sophia Antipolis!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "yagi6glJDr54"
      },
      "outputs": [],
      "source": [
        "txt1 = \"Deep Learning School is fun.\"\n",
        "txt2 = \"Welcome to Sophia Antipolis!\"\n",
        "\n",
        "# your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "7bDpiu4DDr55"
      },
      "source": [
        "You may encounter an error of type `KeyError` about the token `sophia`  because 'Sophia Antipolis' is not in the vocabulary. The next question is about dealing with unknowns tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "aWLtkXfuDr56"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Modify the class by adding a entry to the dictionary corresponding to unknown tokens: `[UNK]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "qwOnfX0sDr6D"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "hidden": true,
        "id": "ZwXOnruRDr6D"
      },
      "outputs": [],
      "source": [
        "# hint 1: outside the class\n",
        "token2id['[UNK]'] = len(token2id)\n",
        "id2token[len(id2token)] = '[UNK]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "hidden": true,
        "id": "8Yp9o8vYDr6E"
      },
      "outputs": [],
      "source": [
        "# hint 2: encoding modified\n",
        "# Add a step that replace every unknown token in textSplitted by [UNK]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-Tk0MvBWDr6G"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Decode the two encoded texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "VH1oLs48Dr6G"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "ErBO1dltDr6H"
      },
      "source": [
        "# BPE Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "H9kWxYUHDr6H"
      },
      "source": [
        "## Vocabulary for the BPE algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "qyVkQJN5e8FT"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> For the BPE used in GPT2, spaces have been replaced by the character 'Ä ' while these spaces have remain unchanged in more recent versions of GPT. For clarity in this exercise, will keep the  'Ä ' in replacement of spaces. The vocabulary is initialized by all letters and symbols appearing in the corpus of texts. This time, we do not split by word, but by character: the initial vocabulary might look like [\"[EOT]\", \"!\", \",\", \"0\", \"a\", \"b\", \"Ä \"] (this is an example, not the real answer). Let's start with the previous corpus and compute the vocabulary!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "lkctuxt1Dr6I"
      },
      "outputs": [],
      "source": [
        "corpus = [text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "4R3deHVIgTvc"
      },
      "outputs": [],
      "source": [
        "alphabet = []\n",
        "\n",
        "# your work is to add (only once) all letters and symbols appearing in the corpus\n",
        "# don't forget to add the symbol of end of text.\n",
        "vocab = [\"[UNK]\",\"[EOT]\"] + alphabet.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "cYIhpY_yObVX"
      },
      "source": [
        "## Enriching the corpus\n",
        "We enrich the corpus with a [small english text: The Story of an Hour](https://en.wikisource.org/wiki/The_Story_of_an_Hour) that you will download from [this link](https://opencourses.univ-cotedazur.fr/mod/resource/view.php?id=8460)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Qcuv0z-vO_sd"
      },
      "source": [
        "You can save this file wherever you want (google drive, google colab temporary files, local hard disk...). Select and modify the next cells depending your way of downloading the file (and working)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "iNpTCvyu-vx6",
        "outputId": "42bfb0e6-f111-4765-cedd-062e2d21e9f6",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "# this cell only if you are running this notebook on google colab and need to connect to your drive where you may have files to load.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "bz-edv7cPg-2"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "5DeBeWNze7gt",
        "outputId": "8037ed8b-5f6c-42e7-ed8b-5ef7696ebeec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 5586\n"
          ]
        }
      ],
      "source": [
        "# change according to where you decided to save the text file\n",
        "pathAndFilename = \"./TheStoryOfAnHour.txt\"\n",
        "with open(pathAndFilename, \"r\", encoding=\"utf-8\") as f:\n",
        "    textTSH = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(textTSH))\n",
        "\n",
        "# Let's add this text to the corpus:\n",
        "corpus.append(textTSH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "UzmYUy4pDr6N"
      },
      "source": [
        "You can add later more short stories from [wikisource](https://en.wikisource.org/wiki/Category:Short_story_authors)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "ziRYourzDr6O"
      },
      "source": [
        "### Computing again the BPE vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "MVnZ7fnuDr6O"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "2mu333z2ID2B"
      },
      "source": [
        "## Computing words, pairs and frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "RzEgb9cpDr6Q"
      },
      "source": [
        "### Computing words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "kYRmIlddDr6Q"
      },
      "source": [
        "BPE is based on frequencies of pairs of tokens **inside words**. By words, we mean a pre-tokenization of each texts in the corpus. Each text is splitted according to punctuation and spaces. Spaces are replaced by character 'Ä ' and '\\n' are replaced by 'ÄŠ'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yBYmtpjIDr6R"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Compute the list of words by splitting the text into words and adding a 'Ä ' before each word that follow a space character.  Don't forget to lowercase the text.\n",
        "\n",
        "Example: \"Welcome to the 2025 Deep Learning School! Deep Learning is fun. Hope you will learn a lot at the school!\" will be splitted as:\n",
        "<pre>['welcome', 'Ä to', 'Ä the', 'Ä 2025', 'Ä deep', 'Ä learning', 'Ä school', '!', 'Ä deep', 'Ä learning', 'Ä is', 'Ä fun', '.', 'Ä hope', 'Ä you', 'Ä will', 'Ä learn', 'Ä a', 'Ä lot', 'Ä at', 'Ä the', 'Ä school', '!']</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "G1pHSw5mDr6S"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "m-B_ZhPXDr6S"
      },
      "outputs": [],
      "source": [
        "# hint: start using only the first text in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "PiFZOJGEDr6T"
      },
      "outputs": [],
      "source": [
        "# hint 2: a way of replacing multiple space by a single one\n",
        "txt = \" test 1  2   3     4                  8\"\n",
        "re.sub(r'(\\s\\s*)',' ',txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "XsnmaPZYDr6U"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Compute the list of unique words and their occurrences in the whole corpus. `words` is the list of unique words (with \"Ä \" at the beginning if the words followed a space character). `freqs` is the list counting the occurence of each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "VnGf8yZLDr6U"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "g4OPZMOzDr6W"
      },
      "source": [
        "### Pairs inside words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "aDLphA5uDr6X"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Compute all the pairs of tokens inside each of the words and their occurences. For this purpose, we suggest to use the [`defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict) class from `collection` which is a subclass of `dict` with the possibility of default values for missing entries. For example, if the corpus was \"That thing\", the pairs and occurences `pairsANDfreqs` would be the following dictionary:\n",
        "{('t', 'h'): 2\n",
        "('h', 'a'): 1\n",
        "('a', 't'): 1\n",
        "('Ä ', 't'): 1\n",
        "('h', 'i'): 1\n",
        "('i', 'n'): 1\n",
        "('n', 'g'): 1}.\n",
        "Use the real corpus, which is composed of the text from the beginning of the lab and the The_Story_of_an_Hour text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "oLGlzOAdDr6X"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "pairsANDfreqs = defaultdict(int)\n",
        "# your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "46iZf4m3Dr6Y"
      },
      "outputs": [],
      "source": [
        "# CORRECTION\n",
        "from collections import defaultdict\n",
        "pairsANDfreqs = defaultdict(int)\n",
        "for word, freq in zip(words,freqs):\n",
        "    if len(word)>1:\n",
        "        for i in range(len(word)-1):\n",
        "            aPair = (word[i],word[i+1])\n",
        "            pairsANDfreqs[aPair] += freq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "58afvIdMDr6Y",
        "outputId": "58277932-f78e-4d81-bb43-bbf37478f788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('f', 'r'): 11\n",
            "('r', 'e'): 63\n",
            "('e', 'e'): 22\n",
            "('g', 'o'): 2\n",
            "('k', 'i'): 6\n",
            "('i', 'l'): 17\n",
            "('l', 'l'): 26\n",
            "('l', 'e'): 30\n",
            "('e', 'd'): 46\n",
            "('l', 'o'): 24\n",
            "('o', 'u'): 36\n",
            "('u', 'i'): 6\n",
            "('i', 's'): 25\n",
            "('s', 'e'): 38\n",
            "('w', 'e'): 15\n",
            "('e', 'l'): 30\n",
            "('l', 'c'): 2\n",
            "('c', 'o'): 18\n",
            "('o', 'm'): 22\n",
            "('m', 'e'): 26\n",
            "('Ä ', '2'): 1\n"
          ]
        }
      ],
      "source": [
        "for i, key in enumerate(pairsANDfreqs.keys()):\n",
        "    print(f\"{key}: {pairsANDfreqs[key]}\")\n",
        "    if i >= 20: # not displaying everything\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "6L0uueDPDr6Z"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Compute the most frequent pair and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "Ggkq3goUDr6a"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "xSquiiLbDr6a",
        "outputId": "f4e983db-1256-42fc-904e-47bf1a48fb70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most frequent pair: ('h', 'e'): 176\n"
          ]
        }
      ],
      "source": [
        "# CORRECTION\n",
        "fmax = 0\n",
        "pmax = None\n",
        "for p,f in pairsANDfreqs.items():\n",
        "    if pmax == None or f>fmax:\n",
        "        pmax = p\n",
        "        fmax = f\n",
        "print(f\"Most frequent pair: {pmax}: {fmax}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "nPpPkz0bDr6a"
      },
      "source": [
        "### Merging most frequent pair of tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KxDi1h1zDr6b"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Next step in the BPE algorithm is to merge the elements of the most frequent pair into a single elements and to add this element to the vocabulary. Compute the merges to perform, and update the vocabulary. For example, if the corpus was \"That thing\", the most frequent pair would be (\"t\", \"h\"), therefore the merges would be {('t', 'h'): 'th'}, and the vocabulary (that was first initialized by individual character and the \"[EOT]\" token) would then contain the additional \"th\" element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "VdgpJXwGDr6b"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "cIqnQL5NDr6d"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> At this point in the BPE algorithm, words won't be anymore splitted by characters but using the vocabulary that will grow according to most frequent pairs. We then need to build a list of splitted words `splittedWords` and update it with this new merge. For example, if the corpus was \"That thing\", the `splittedWords` would be initialized by {'that': ['t', 'h', 'a', 't'], 'Ä thing': ['Ä ', 't', 'h', 'i', 'n', 'g']}, but since the most frequent pair is \"th\", it would then be modified to become {'that': ['th', 'a', 't'], 'Ä thing': ['Ä ', 'th', 'i', 'n', 'g']}. Here, we will use the real corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "HtVntkveDr6d"
      },
      "outputs": [],
      "source": [
        "# initial splitting of words that should then be modified\n",
        "splittedWords = {word: [c for c in word] for word in words}\n",
        "\n",
        "# for each word containing the elements that have to be merged, modify the splitted version\n",
        "# In our case (real corpus), 'h' and 'e' will be merged as 'e' so that 'the' is not decomposed to 't','h' and 'e' anymore\n",
        "#   but to 't' and 'he'.\n",
        "\n",
        "# your work here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "nqwJ0JvrDr6e"
      },
      "source": [
        "### BPE loop: refactoring code into functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "vxtMX7ofDr6e"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> In the BPE encoding algorithm, we will loop on finding the most frequent pair, merging the elements of the pair and updating the splitting of each words. For this convenience, we will rewrite the previous questions using functions. As an example, you could have a function for: computing the list of words in a text, computing the list of unique words with the list of their occurence in a corpus of text, initializing the vocabulary with individual letters and \"<|endoftext|>\" token, computing the pairs and their occurences in words, merging a pair, computing the most frequent pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "46k7loIZDr6f"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "9bOz3_gSDr6g"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Write the loop that computes the most frequent pairs and merges until a fix size of the vocabulary is reached (then, we stop the merges)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "TKV5eWk0Dr6g"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "2MNBIclfDr6h"
      },
      "source": [
        "## Refactoring the code into a BPETokenizer class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "6GkHnZsKDr6i"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> You are know ready to write the second tokenizer class of this lab: the `BPETokenizer` class. With the help of the previously defined functions, fill the body of the following class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "RsMntX0sDr6i"
      },
      "outputs": [],
      "source": [
        "# your work\n",
        "class BPETokenizer:\n",
        "    VOCAB_MAX_LENGTH = 100 # for construction\n",
        "\n",
        "    # initialisation from a corpus of texts, building vocabulary and merges\n",
        "    def __init__(self, corpus):\n",
        "\n",
        "    # encode a single text\n",
        "    def encode(self, text):\n",
        "\n",
        "    # decode an encoded text\n",
        "    def decode(self, ids):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "EyalyPxaDr6k",
        "outputId": "2a5696e6-bda5-4c6e-ed66-0a5156ebb400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoded:  [35, 73, 15, 81, 17, 63, 17, 17, 28, 64, 17, 75, 26, 57, 43, 15, 20, 27, 27, 24]\n",
            "decoded:  welcome deep learning school\n"
          ]
        }
      ],
      "source": [
        "myTokenizer = BPETokenizer(corpus)\n",
        "encoded = myTokenizer.encode(\"Welcome Deep Learning School\")\n",
        "print('encoded: ', encoded)\n",
        "decoded = myTokenizer.decode(encoded)\n",
        "print('decoded: ', decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "DYh8nv28Dr6k"
      },
      "source": [
        "### Byte-Level BPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3x-WN_cRDr6l"
      },
      "source": [
        "So far, we have worked using letters and groups of letters so that the output was easy to read for a human. The Byte-Level BPE consider the bytes of the utf-8 encoding of words. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "oLn2YSZVDr6l",
        "outputId": "526b5f83-be85-4e71-99d9-56b9861433fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'Voil\\xc3\\xa0'\n",
            "[86, 111, 105, 108, 195, 160]\n",
            "b'Voila'\n",
            "[86, 111, 105, 108, 97]\n"
          ]
        }
      ],
      "source": [
        "tokens = (\"VoilÃ \").encode(\"utf-8\")\n",
        "print(tokens)\n",
        "tokens = list(map(int, tokens))\n",
        "print(tokens)\n",
        "\n",
        "tokens = (\"Voila\").encode(\"utf-8\")\n",
        "print(tokens)\n",
        "tokens = list(map(int, tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "7E_Iw7mTDr6m"
      },
      "source": [
        "The initial vocabulary is then composed of the 256 possible values for 1 byte. Characters such as 'Ã ' that are decomposed using 2 bytes in UTF-8 will be first encoded using 2 tokens encodings. If they appear frequently, they might be merged as a pair that could also be merged with another one. However, 160 and 195 may be merged independently with other values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "xK2QmlG1t7_t",
        "outputId": "1fdd2c3c-5fe0-437b-d78f-125301aa9385"
      },
      "source": [
        "ðŸ¤” <b><font color='purple'>Question:</font></b> Write the Byte-level BPE tokenizer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "4ysBsi1cDr6n"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "d2HQVUKL4SEz"
      },
      "source": [
        "## Implementations of BPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ZS8IagBrDr6o"
      },
      "source": [
        "Many library and also many tokenizer algorithms exist. We present here 2 main implementation of BPE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "mPpZ70lzDr6o"
      },
      "source": [
        "### Implementation from the `tiktoken` library\n",
        "\n",
        "First play with the [tiktoken interactive application](https://tiktokenizer.vercel.app/?model=gpt2)!\n",
        "\n",
        "![tiktoken screenshot](http://www.i3s.unice.fr/~lingrand/screenShotTiktoken.png)\n",
        "\n",
        "Then code using the python library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "qQ3oHg8f4wT9",
        "outputId": "0491820d-1da0-4139-b591-0aa9dbef87fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[14618, 284, 262, 32190, 10766, 18252, 3961, 0]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "gpt2_tokenizer.encode(\"Welcome to the 2025 Deep Learning School!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "JegrvwL5Dr6p"
      },
      "source": [
        "### Implementation from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "1_LK3qV1Dr6p"
      },
      "outputs": [],
      "source": [
        "# If necessary:\n",
        "!pip install transformers[sentencepiece]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "CaM83lkHDr6q"
      },
      "outputs": [],
      "source": [
        "# Loading the GPT2 tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.encode(\"Welcome to the 2025 Deep Learning School!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cb5d5e7",
        "outputId": "b8bc1567-a2ff-4376-e8fb-6de2cab6ef73"
      },
      "source": [
        "\n",
        "decoded_text_improved = \"\"\n",
        "for i, token_id in enumerate(encoded_text):\n",
        "    token = id2token[token_id]\n",
        "    # Simple rule: Add a space before most tokens, except for punctuation that usually follows a word.\n",
        "    if token not in ['.', ',', '!', '?', ':', ';', \"'s\", \"'t\", \"'ll\", \"'d\", \"'ve\", \"'re\", \"'m\"] and i > 0:\n",
        "        decoded_text_improved += \" \"\n",
        "    decoded_text_improved += token\n",
        "\n",
        "# Capitalize the first letter and capitalize after periods.\n",
        "decoded_text_improved = decoded_text_improved.capitalize()\n",
        "sentences = decoded_text_improved.split('. ')\n",
        "decoded_text_improved = '. '.join([sentence.capitalize() for sentence in sentences])\n",
        "\n",
        "print(decoded_text_improved)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the 2025 deep learning school  deep learning is fun  hope you will learn a lot at the school \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_UZE0OLN6O4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}