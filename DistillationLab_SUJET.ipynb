{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabal5ghosh/Deep-Learning-summer-school-2025-university-of-cote-d-Azur/blob/main/DistillationLab_SUJET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W9nOhv4g0FU"
      },
      "source": [
        "<center><img width=60% src=\"http://www.i3s.unice.fr/~lingrand/efeliaUnica.png\"><br/><br/>\n",
        "<font size=+3><b>Distillation in neural networks</b></font><br/><br/>\n",
        "<font size=+1>C√©lia D'cruz, R√©my Sun, Fr√©d√©ric Precioso and Diane Lingrand<br/>\n",
        "Deep Learning School<br/>\n",
        "    2025 - June/July<br/></font>\n",
        "    <img width=14% src=\"http://www.i3s.unice.fr/~lingrand/cc-long.png\">\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byLfVNlBAP0a"
      },
      "source": [
        "<font size=+2> This lab is based on the pytorch lab on [Knowledge Distillation Tutorial by Alexandros Chariton](https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_-k76WhAP0b"
      },
      "source": [
        "Knowledge distillation is a technique that enables knowledge transfer\n",
        "from large, computationally expensive models to smaller ones without\n",
        "losing validity. This allows for deployment on less powerful hardware,\n",
        "making evaluation faster and more efficient.\n",
        "\n",
        "In this tutorial, we will run a number of experiments focused at\n",
        "improving the accuracy of a lightweight neural network, using a more\n",
        "powerful network as a teacher. The computational cost and the speed of\n",
        "the lightweight network will remain unaffected, our intervention only\n",
        "focuses on its weights, not on its forward pass. Applications of this\n",
        "technology can be found in devices such as drones or mobile phones. In\n",
        "this tutorial, we do not use any external packages as everything we need\n",
        "is available in `torch` and `torchvision`.\n",
        "\n",
        "In this tutorial, you will learn:\n",
        "\n",
        "-   How to modify model classes to extract hidden representations and\n",
        "    use them for further calculations\n",
        "-   How to modify regular train loops in PyTorch to include additional\n",
        "    losses on top of, for example, cross-entropy for classification\n",
        "-   How to improve the performance of lightweight models by using more\n",
        "    complex models as teachers\n",
        "\n",
        "Prerequisites\n",
        "=============\n",
        "\n",
        "-   1 GPU, 4GB of memory\n",
        "-   PyTorch v2.0 or later\n",
        "-   CIFAR-10 dataset (downloaded by the script and saved in a directory\n",
        "    called `/data`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x9HemfKoAP0b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F4cOOuig0FZ",
        "outputId": "7e38b5b7-51c7-4704-e56c-a1e82ac94a92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# making the code device agnostic\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQd2DWpbAP0c"
      },
      "source": [
        "Loading CIFAR-10\n",
        "================\n",
        "\n",
        "CIFAR-10 is a popular image dataset with ten classes. Our objective is\n",
        "to predict one of the following classes for each input image.\n",
        "\n",
        "![Example of CIFAR-10images](https://pytorch.org/tutorials//../_static/img/cifar10.png)\n",
        "\n",
        "The input images are RGB, so they have 3 channels and are 32x32 pixels.\n",
        "Basically, each image is described by 3 x 32 x 32 = 3072 numbers ranging\n",
        "from 0 to 255. A common practice in neural networks is to normalize the\n",
        "input, which is done for multiple reasons, including avoiding saturation\n",
        "in commonly used activation functions and increasing numerical\n",
        "stability. Our normalization process consists of subtracting the mean\n",
        "and dividing by the standard deviation along each channel. The tensors\n",
        "\\\"mean=\\[0.485, 0.456, 0.406\\]\\\" and \\\"std=\\[0.229, 0.224, 0.225\\]\\\"\n",
        "were already computed, and they represent the mean and standard\n",
        "deviation of each channel in the predefined subset of CIFAR-10 intended\n",
        "to be the training set. Notice how we use these values for the test set\n",
        "as well, without recomputing the mean and standard deviation from\n",
        "scratch. This is because the network was trained on features produced by\n",
        "subtracting and dividing the numbers above, and we want to maintain\n",
        "consistency. Furthermore, in real life, we would not be able to compute\n",
        "the mean and standard deviation of the test set since, under our\n",
        "assumptions, this data would not be accessible at that point.\n",
        "\n",
        "As a closing point, we often refer to this held-out set as the\n",
        "validation set, and we use a separate set, called the test set, after\n",
        "optimizing a model\\'s performance on the validation set. This is done to\n",
        "avoid selecting a model based on the greedy and biased optimization of a\n",
        "single metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nHHCNJkzAP0e",
        "outputId": "3a554157-025c-46b9-af7a-4ab8e3b73b49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:13<00:00, 12.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "transforms_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 dataset:\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-QLGP37sePK_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "train_dataset, val_dataset = random_split(train_dataset, [40000, 10000], torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KsB2JjhAP0e"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>This section is for CPU users only who are interested in quick results. Use this option only if you're interested in a small scale experiment. Keep in mind the code should run fairly quickly using any GPU. Select only the first <code>num_images_to_keep</code> images from the train/test dataset<pre><code>#from torch.utils.data import Subset</p>\n",
        "<h1>num_images_to_keep = 2000</h1>\n",
        "<h1>train_dataset = Subset(train_dataset, range(min(num_images_to_keep, 50_000)))</h1>\n",
        "<h1>test_dataset = Subset(test_dataset, range(min(num_images_to_keep, 10_000)))</code></pre></h1>\n",
        "```\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eC1TdDmdAP0f"
      },
      "outputs": [],
      "source": [
        "#Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw-fVWaaePLB"
      },
      "source": [
        "ü§î <b><font color='purple'>Question:</font></b> How many samples are there in the train and test set? Print one sample from the train set: what is the shape of the input image, what is its associated label (class)?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train set shape\", len(train_dataset))\n",
        "print(\"test set shape\", len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5zB3LLdtEBN",
        "outputId": "8be8e51c-7b48-4a65-fcd7-92328b15cfbe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set shape 40000\n",
            "test set shape 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "one_sample_input, one_sample_target = train_dataset[0]\n",
        "print(\"one image =\", one_sample_input)\n",
        "print(\"shape of the image =\", one_sample_input.shape)\n",
        "print(\"image label =\", one_sample_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG6esUDutuor",
        "outputId": "e2dc0a55-1c77-4653-f16c-88679ad02825"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one image = tensor([[[-1.0733, -0.9020, -0.7822,  ..., -0.7822, -0.9020, -1.1247],\n",
            "         [-0.1657, -0.2513, -0.5253,  ..., -0.7137, -0.9020, -1.0904],\n",
            "         [-0.7479, -0.9877, -1.2788,  ..., -0.5082, -0.5424, -0.7308],\n",
            "         ...,\n",
            "         [-2.0494, -2.0152, -2.1179,  ..., -0.1828, -0.3369, -0.1828],\n",
            "         [-1.9809, -1.8782, -2.0152,  ..., -0.0972, -0.4568, -0.5767],\n",
            "         [-1.4672, -1.4672, -1.6213,  ..., -0.4739, -0.6794, -0.5596]],\n",
            "\n",
            "        [[-1.0378, -0.9503, -0.8102,  ..., -0.8978, -1.0203, -1.2129],\n",
            "         [-0.1275, -0.2850, -0.5126,  ..., -0.7577, -0.9678, -1.1429],\n",
            "         [-0.7052, -0.9853, -1.1779,  ..., -0.4951, -0.5826, -0.7752],\n",
            "         ...,\n",
            "         [-1.7206, -1.8081, -1.7206,  ..., -0.7052, -1.0553, -1.1779],\n",
            "         [-1.8256, -1.7731, -1.8081,  ..., -0.6877, -1.0028, -1.2479],\n",
            "         [-1.5280, -1.5980, -1.7206,  ..., -1.1429, -1.3004, -1.1779]],\n",
            "\n",
            "        [[-0.8284, -0.7587, -0.6541,  ..., -0.9156, -0.9678, -1.1421],\n",
            "         [ 0.1476,  0.0256, -0.2881,  ..., -0.8110, -0.9330, -1.0550],\n",
            "         [-0.5844, -0.8633, -1.2119,  ..., -0.4973, -0.5147, -0.6541],\n",
            "         ...,\n",
            "         [-1.2990, -1.5256, -1.7173,  ..., -0.7064, -1.0201, -1.2641],\n",
            "         [-1.4907, -1.4733, -1.5953,  ..., -0.6367, -0.9678, -1.2467],\n",
            "         [-1.2467, -1.2816, -1.4036,  ..., -1.0201, -1.1944, -1.1073]]])\n",
            "shape of the image = torch.Size([3, 32, 32])\n",
            "image label = 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qEoOXEwyePLC"
      },
      "outputs": [],
      "source": [
        "# your work\n",
        "train_loader_iter = iter(train_loader)\n",
        "train_images, train_labels = next(train_loader_iter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2KsqOebTKow",
        "outputId": "4a82a3af-1e07-4580-d14d-dd552ee00a79"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40000"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN8nit8uTPcO",
        "outputId": "5e4f59f9-58dc-4b2f-e127-dbb06a9bf0b0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1.3413,  1.3413,  1.3242,  ...,  1.3584,  1.2043,  1.2899],\n",
              "          [ 1.2214,  1.1872,  1.2043,  ...,  0.9988,  0.9132,  1.0159],\n",
              "          [ 1.0159,  0.9646,  0.9817,  ...,  0.7762,  0.7762,  0.9303],\n",
              "          ...,\n",
              "          [-1.5699, -1.4158, -1.0390,  ..., -1.7069, -1.7412, -1.7583],\n",
              "          [-1.5185, -0.9363,  0.0912,  ..., -1.6213, -1.7754, -1.7412],\n",
              "          [-1.5699, -1.4843, -1.2445,  ..., -1.7412, -1.7754, -1.7240]],\n",
              "\n",
              "         [[ 1.9209,  1.9209,  1.8859,  ...,  1.9559,  1.8333,  1.8859],\n",
              "          [ 1.8333,  1.8158,  1.8508,  ...,  1.7808,  1.7108,  1.7108],\n",
              "          [ 1.7458,  1.7283,  1.7458,  ...,  1.6933,  1.6933,  1.6933],\n",
              "          ...,\n",
              "          [-0.7227, -0.6176, -0.3901,  ..., -0.3550, -0.3901, -0.4076],\n",
              "          [-0.6352, -0.1975,  0.6604,  ..., -0.3200, -0.3200, -0.5301],\n",
              "          [-0.7752, -0.7927, -0.6527,  ..., -0.8102, -0.6176, -0.7227]],\n",
              "\n",
              "         [[ 2.6400,  2.5703,  2.6051,  ...,  2.6226,  2.6051,  2.6226],\n",
              "          [ 2.5180,  2.4134,  2.3786,  ...,  2.3960,  2.3960,  2.4831],\n",
              "          [ 2.6051,  2.5180,  2.4657,  ...,  2.4483,  2.4483,  2.5529],\n",
              "          ...,\n",
              "          [ 0.3568,  0.2696,  0.3568,  ...,  0.5485,  0.5311,  0.6879],\n",
              "          [ 0.3568,  0.6879,  1.0191,  ...,  0.4439,  0.4614,  0.6356],\n",
              "          [ 0.8971,  0.9494,  0.7054,  ...,  0.6531,  0.8099,  0.9145]]],\n",
              "\n",
              "\n",
              "        [[[ 1.9235,  1.8893,  1.8722,  ...,  2.0948,  2.0948,  2.0948],\n",
              "          [ 1.9407,  1.9578,  1.8379,  ...,  2.1462,  2.1462,  2.1290],\n",
              "          [ 2.0948,  2.0605,  2.0263,  ...,  2.2147,  2.1804,  2.1119],\n",
              "          ...,\n",
              "          [-1.1932, -1.2274, -1.2445,  ..., -1.6042, -1.6042, -1.5528],\n",
              "          [-1.3302, -1.3473, -1.3644,  ..., -1.5699, -1.5357, -1.5185],\n",
              "          [-1.0219, -0.9877, -0.8678,  ..., -1.5185, -1.5014, -1.5014]],\n",
              "\n",
              "         [[ 2.3761,  2.3585,  2.3410,  ...,  2.4286,  2.4286,  2.4286],\n",
              "          [ 2.3410,  2.3410,  2.2360,  ...,  2.4111,  2.4111,  2.4286],\n",
              "          [ 2.4286,  2.3936,  2.3761,  ...,  2.4111,  2.4111,  2.4111],\n",
              "          ...,\n",
              "          [-0.7577, -0.7752, -0.7752,  ..., -1.1604, -1.0728, -0.9853],\n",
              "          [-0.8627, -0.8978, -0.8978,  ..., -1.0378, -0.9678, -0.9153],\n",
              "          [-0.5126, -0.4951, -0.3550,  ..., -0.9503, -0.9153, -0.8803]],\n",
              "\n",
              "         [[ 2.6400,  2.6400,  2.6226,  ...,  2.6400,  2.6400,  2.6400],\n",
              "          [ 2.6226,  2.6051,  2.4831,  ...,  2.5703,  2.6051,  2.6226],\n",
              "          [ 2.6400,  2.6226,  2.5877,  ...,  2.5703,  2.6226,  2.6400],\n",
              "          ...,\n",
              "          [ 0.2348,  0.2173,  0.2173,  ..., -0.2532, -0.0441,  0.1825],\n",
              "          [ 0.1999,  0.1651,  0.1651,  ..., -0.0615,  0.0431,  0.1476],\n",
              "          [ 0.4614,  0.4962,  0.6008,  ...,  0.0779,  0.1302,  0.1825]]],\n",
              "\n",
              "\n",
              "        [[[-0.1486, -0.1143, -0.1828,  ...,  0.0398,  0.0569, -0.0629],\n",
              "          [ 0.1083, -0.1486, -0.3027,  ...,  0.0056,  0.2967,  0.1083],\n",
              "          [ 0.1597,  0.0569, -0.1486,  ..., -0.1314,  0.2282,  0.1597],\n",
              "          ...,\n",
              "          [-0.1143,  0.1939,  0.2967,  ..., -0.2513, -0.5938, -0.6623],\n",
              "          [-0.2171, -0.0287,  0.2967,  ..., -0.3027, -0.4739, -0.5767],\n",
              "          [-0.1143, -0.0116,  0.2624,  ..., -0.1828, -0.1314, -0.1828]],\n",
              "\n",
              "         [[-0.0399,  0.0126, -0.0574,  ...,  0.0476,  0.0126, -0.0399],\n",
              "          [ 0.2402, -0.0224, -0.1800,  ...,  0.0126,  0.2752,  0.1352],\n",
              "          [ 0.2577,  0.1527, -0.0224,  ..., -0.0924,  0.2227,  0.2052],\n",
              "          ...,\n",
              "          [-0.1275,  0.1877,  0.2927,  ..., -0.1275, -0.4776, -0.5476],\n",
              "          [-0.2325, -0.0399,  0.2927,  ..., -0.1800, -0.3550, -0.4601],\n",
              "          [-0.1275, -0.0224,  0.2577,  ..., -0.0574, -0.0049, -0.0574]],\n",
              "\n",
              "         [[ 0.1999,  0.2348,  0.1651,  ...,  0.3219,  0.3045,  0.1999],\n",
              "          [ 0.4614,  0.1999,  0.0431,  ...,  0.2871,  0.5485,  0.3568],\n",
              "          [ 0.5311,  0.4091,  0.1825,  ...,  0.1476,  0.4788,  0.4439],\n",
              "          ...,\n",
              "          [ 0.0256,  0.3568,  0.4614,  ...,  0.1302, -0.2707, -0.3230],\n",
              "          [-0.0615,  0.1302,  0.4614,  ...,  0.0779, -0.1312, -0.2358],\n",
              "          [ 0.0431,  0.1476,  0.4265,  ...,  0.1999,  0.2173,  0.1651]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[-2.0323, -1.9638, -2.0494,  ..., -1.9638, -1.9980, -2.0494],\n",
              "          [-1.5528, -0.7308, -0.7993,  ..., -1.3473, -1.3473, -1.7240],\n",
              "          [-1.2788,  0.0741,  0.1597,  ..., -1.1932, -1.1589, -1.6042],\n",
              "          ...,\n",
              "          [-1.1760,  0.4166,  0.4337,  ...,  0.5193,  0.4166, -1.1589],\n",
              "          [-1.3815, -0.0972, -0.0629,  ..., -0.1143, -0.1657, -1.3987],\n",
              "          [-1.9467, -1.7754, -1.7583,  ..., -1.2788, -1.3987, -1.8268]],\n",
              "\n",
              "         [[-1.9832, -1.9657, -1.9482,  ..., -1.9482, -1.9307, -1.9482],\n",
              "          [-1.5455, -0.7752, -0.7752,  ..., -1.3354, -1.3179, -1.6506],\n",
              "          [-1.3004, -0.0224,  0.1001,  ..., -1.2304, -1.1779, -1.5630],\n",
              "          ...,\n",
              "          [-1.1253,  0.4678,  0.4503,  ...,  0.5378,  0.4503, -1.1253],\n",
              "          [-1.3004, -0.0574, -0.0399,  ..., -0.1099, -0.1275, -1.3354],\n",
              "          [-1.8606, -1.7206, -1.7381,  ..., -1.2479, -1.3529, -1.7556]],\n",
              "\n",
              "         [[-1.7522, -1.6999, -1.6999,  ..., -1.7347, -1.7173, -1.6999],\n",
              "          [-1.3687, -0.6890, -0.7064,  ..., -1.1770, -1.1421, -1.4384],\n",
              "          [-1.2119, -0.1661, -0.0441,  ..., -1.1247, -1.0376, -1.3687],\n",
              "          ...,\n",
              "          [-1.0027,  0.4962,  0.4439,  ...,  0.4788,  0.4614, -0.9853],\n",
              "          [-1.0724,  0.1302,  0.0953,  ...,  0.0256,  0.0431, -1.1073],\n",
              "          [-1.5779, -1.4559, -1.5081,  ..., -1.0027, -1.0898, -1.4733]]],\n",
              "\n",
              "\n",
              "        [[[ 1.1187,  1.0673,  0.8618,  ...,  0.2282, -0.0801,  0.6734],\n",
              "          [ 1.3242,  1.2214,  0.9132,  ..., -0.7137, -0.4054,  0.4337],\n",
              "          [ 1.2899,  1.4954,  1.0673,  ...,  0.1939,  0.3481,  0.6049],\n",
              "          ...,\n",
              "          [-1.8782, -1.9467, -1.9295,  ..., -0.1143, -0.3712, -0.3369],\n",
              "          [-1.0904, -0.9705, -0.7650,  ...,  0.0912, -0.1999, -0.2856],\n",
              "          [ 0.0398,  0.0741,  0.1768,  ...,  0.2796,  0.0056, -0.0972]],\n",
              "\n",
              "         [[ 1.0105,  0.9580,  0.7304,  ...,  0.1527, -0.2500,  0.5028],\n",
              "          [ 1.2381,  1.1155,  0.8179,  ..., -0.6176, -0.5126,  0.2927],\n",
              "          [ 1.2031,  1.4132,  0.9930,  ...,  0.3627,  0.3452,  0.5378],\n",
              "          ...,\n",
              "          [-1.8256, -1.8957, -1.8782,  ...,  0.3277,  0.0301,  0.0301],\n",
              "          [-1.0028, -0.8627, -0.6527,  ...,  0.4678,  0.2052,  0.1176],\n",
              "          [ 0.1527,  0.1877,  0.2927,  ...,  0.6078,  0.4153,  0.3102]],\n",
              "\n",
              "         [[ 1.1062,  1.0539,  0.8274,  ...,  0.4788,  0.0256,  0.7228],\n",
              "          [ 1.2980,  1.1934,  0.8797,  ..., -0.1487, -0.2010,  0.5136],\n",
              "          [ 1.2108,  1.4200,  1.0017,  ...,  0.8099,  0.5659,  0.6182],\n",
              "          ...,\n",
              "          [-1.5430, -1.6127, -1.5953,  ..., -0.1487, -0.3055, -0.2358],\n",
              "          [-0.8458, -0.7587, -0.5844,  ...,  0.1128, -0.1312, -0.2010],\n",
              "          [ 0.1651,  0.1825,  0.2522,  ...,  0.3219,  0.0779, -0.0092]]],\n",
              "\n",
              "\n",
              "        [[[ 1.8037,  1.9920,  2.1975,  ...,  2.2147,  2.2147,  2.2147],\n",
              "          [ 1.5639,  1.9064,  2.1975,  ...,  2.2489,  2.2489,  2.2489],\n",
              "          [ 1.4269,  1.7180,  2.0092,  ...,  2.2318,  2.2318,  2.2318],\n",
              "          ...,\n",
              "          [-0.7993, -0.7308, -0.8164,  ..., -0.2171,  0.9132,  0.7933],\n",
              "          [-0.8678, -0.9020, -0.9363,  ...,  0.3652,  0.9988,  0.7591],\n",
              "          [-0.8335, -0.9020, -0.8507,  ...,  0.7933,  1.2385,  0.9474]],\n",
              "\n",
              "         [[ 1.9559,  2.1485,  2.3585,  ...,  2.3936,  2.3936,  2.3936],\n",
              "          [ 1.6232,  2.0084,  2.3235,  ...,  2.4286,  2.4286,  2.4286],\n",
              "          [ 1.3957,  1.7458,  2.0959,  ...,  2.4111,  2.4111,  2.4111],\n",
              "          ...,\n",
              "          [-0.8803, -0.8102, -0.8978,  ..., -0.0049,  0.8704,  0.7829],\n",
              "          [-0.9503, -0.9853, -1.0203,  ...,  0.3452,  0.9055,  0.8004],\n",
              "          [-0.9153, -0.9678, -0.9153,  ...,  0.6604,  1.0280,  0.9230]],\n",
              "\n",
              "         [[ 1.7860,  2.2914,  2.3960,  ...,  2.6051,  2.6051,  2.6051],\n",
              "          [ 1.2108,  1.9951,  2.3437,  ...,  2.6400,  2.6400,  2.6400],\n",
              "          [ 0.7402,  1.5071,  1.9951,  ...,  2.6226,  2.6226,  2.6226],\n",
              "          ...,\n",
              "          [-0.3753, -0.3055, -0.3927,  ...,  0.4439,  1.1585,  0.9668],\n",
              "          [-0.4450, -0.4798, -0.5147,  ...,  0.5485,  1.1237,  1.0191],\n",
              "          [-0.4101, -0.4798, -0.4275,  ...,  0.7576,  1.1585,  1.0888]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGZUj_RuoYjs",
        "outputId": "d45eae43-dcf2-48b8-a536-75fd5f51c54d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_images[0].permute(1, 2, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "oH_puyJMTPYl",
        "outputId": "854dd444-7775-4b9c-a214-b8f46adfd5e5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.117904..2.64].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x793f3a5c7890>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader_iter = iter(test_loader)\n",
        "test_images, test_labels = next(test_loader_iter)"
      ],
      "metadata": {
        "id": "Mba37G8pprTU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-geE0j7qVo3",
        "outputId": "18bafada-b102-4e5d-ddd8-90a36b17d7f0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uLgDhtvp_nt",
        "outputId": "f2a68d1b-e928-412a-972a-b5c884c27d69"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_images[0].permute(1, 2, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "AnFQfabAp_j5",
        "outputId": "521d3a0b-126f-4c4d-d941-b30d33f1f4f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8952821..2.5877128].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x793f631aed10>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyg-O97MePLC"
      },
      "source": [
        "We plot the distribution of the train, valid and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "POBcjAyYePLD"
      },
      "outputs": [],
      "source": [
        "def plot_class_distribution(y_train, y_val, y_test):\n",
        "    number_of_classes = len(np.unique(y_train))\n",
        "\n",
        "    fig, ax = plt.subplots(layout=\"constrained\")\n",
        "    for y, name, offset in zip([y_train, y_val, y_test], [\"Train\", \"Val\", \"Test\"], [-0.2, 0, 0.2]):\n",
        "        rects = ax.bar(np.arange(number_of_classes)+offset, np.bincount(y), 0.2, label=name)\n",
        "        ax.bar_label(rects)\n",
        "\n",
        "    ax.set_ylabel(\"Number of samples\")\n",
        "    ax.set_xlabel(\"Class number\")\n",
        "    ax.set_title(\"Distribution of classes\")\n",
        "    ax.set_xticks(np.arange(number_of_classes))\n",
        "    ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i1K1G6MePLD",
        "outputId": "71926365-d7b8-49f0-8bc4-a4ad303a4472"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_class_distribution(train_dataset.dataset.targets, val_dataset.dataset.targets, test_dataset.targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6V6cFAcAP0g"
      },
      "source": [
        "Defining model classes and utility functions\n",
        "============================================\n",
        "\n",
        "Next, we need to define our model classes. Several user-defined\n",
        "parameters need to be set here. We use two different architectures,\n",
        "keeping the number of filters fixed across our experiments to ensure\n",
        "fair comparisons. Both architectures are Convolutional Neural Networks\n",
        "(CNNs) with a different number of convolutional layers that serve as\n",
        "feature extractors, followed by a classifier with 10 classes. The number\n",
        "of filters and neurons is smaller for the students.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "k_DdYvxLAP0h"
      },
      "outputs": [],
      "source": [
        "# Deeper neural network class to be used as teacher:\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Lightweight neural network class to be used as student:\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt8x6YWGAP0i"
      },
      "source": [
        "We employ 2 functions to help us produce and evaluate the results on our\n",
        "original classification task. One function is called `train` and takes\n",
        "the following arguments:\n",
        "\n",
        "-   `model`: A model instance to train (update its weights) via this\n",
        "    function.\n",
        "-   `train_loader`: We defined our `train_loader` above, and its job is\n",
        "    to feed the data into the model.\n",
        "-   `epochs`: How many times we loop over the dataset.\n",
        "-   `learning_rate`: The learning rate determines how large our steps\n",
        "    towards convergence should be. Too large or too small steps can be\n",
        "    detrimental.\n",
        "-   `device`: Determines the device to run the workload on. Can be\n",
        "    either CPU or GPU depending on availability.\n",
        "\n",
        "Our test function is similar, but it will be invoked with `test_loader`\n",
        "to load images from the test set.\n",
        "\n",
        "![Train both networks with Cross-Entropy. The student will be used as a\n",
        "baseline:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/ce_only.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "e3UBqDs-AP0i"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        running_val_loss = 0.0\n",
        "        for inputs, labels in val_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "              outputs = model(inputs)\n",
        "\n",
        "              # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "              # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)},  Val_Loss: {running_val_loss / len(val_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTktY1_OePLF"
      },
      "source": [
        "ü§î <b><font color='purple'>Question:</font></b> Add the display of the train and validation accuracies at the end of the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av46Flw4ePLF"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3AMSmqdg0Fe"
      },
      "source": [
        "# Training the teacher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiAg6etAAP0j"
      },
      "source": [
        "Start by training the teacher network using cross-entropy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6On08VZ8AP0j",
        "outputId": "8f60b5a9-b612-4f4f-9490-f2904935464b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.4561911971805195,  Val_Loss: 1.1948809510544887\n",
            "Epoch 2/10, Loss: 0.9847455276087069,  Val_Loss: 0.9154411901401568\n",
            "Epoch 3/10, Loss: 0.7687419260652683,  Val_Loss: 0.8016349726085421\n",
            "Epoch 4/10, Loss: 0.6142794479386875,  Val_Loss: 0.7648729915860333\n",
            "Epoch 5/10, Loss: 0.46846803622885635,  Val_Loss: 0.7939900552170186\n",
            "Epoch 6/10, Loss: 0.34880500293958683,  Val_Loss: 0.8861446207082724\n",
            "Epoch 7/10, Loss: 0.24613572876103007,  Val_Loss: 0.9646181094495556\n",
            "Epoch 8/10, Loss: 0.17579482424373444,  Val_Loss: 1.1543857451481154\n",
            "Epoch 9/10, Loss: 0.14229806723257604,  Val_Loss: 1.2670872445347943\n",
            "Epoch 10/10, Loss: 0.1190516323374864,  Val_Loss: 1.3403417505795443\n",
            "Test Accuracy: 72.83%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "nn_deep = DeepNN(num_classes=10).to(device)\n",
        "train(nn_deep, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KtqD5k_k7tD"
      },
      "source": [
        "As you can observe, while the model clearly improves its training loss throughout training, the validation loss stops improving very fast and in fact starts deteriorating quite early. This phenomenon is called overfitting: the model finds improvements that specifically fit the training dataset but the not the overall distribution of interest.\n",
        "\n",
        "A common way explain such a phenomenon comes from comparing the amount of training data available to the capacity of the model to fit complex functions. If we are dealing with a fairly small amount of training data, even slightly complex models like our DeepNN are going to quickly overfit to it. Unfortunately, acquiring more data is typically difficult.\n",
        "\n",
        "One solution to this problem is to make it harder to overfit to the limited amount of training data available by creating artificial samples from the data we do have: this is called data augmentation. Thankfully for us, pytorch natively supports a wide catalog of transformation functions through its transforms API.\n",
        "\n",
        "ü§î <b><font color='purple'>Question:</font></b> Modify the training dataset transform function to incorporate classical Data Augmentation techniques like Random Cropping and Flipping. You can then retrain the model to see how much this improves the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "OqC88a9jm7SA"
      },
      "outputs": [],
      "source": [
        "# Add the random data augmentation that you want for training.\n",
        "train_transforms_cifar = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "transforms_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 dataset:\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [40000, 10000], torch.Generator().manual_seed(42))\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms_cifar)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)\n",
        "\n",
        "#Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RRqZ70YoMsD",
        "outputId": "86485038-efc6-4eb5-889d-66552575a516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.5692465088861374,  Val_Loss: 1.221982691106917\n",
            "Epoch 2/10, Loss: 1.1661315074052347,  Val_Loss: 0.9518082556845266\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "nn_deep = DeepNN(num_classes=10).to(device)\n",
        "train(nn_deep, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dla5jcqqv-Q"
      },
      "source": [
        "If you want to go further, there are stronger families of data augmentations such as strong data augmentations (similar to what you have been playing with but using much stronger perturbation intensities) and mixing sample data augmentation (where multiple samples are mixed and a hybrid label is predicted).\n",
        "\n",
        "One classical example of a mixing augmentation is the seminal MixUp framework where two images $x_1$ and $x_2$ are interpolated into a mixed sample $x = \\lambda x_1 + (1-\\lambda) x_2$ with typically ratio $\\lambda \\sim \\beta(\\alpha=1, \\alpha=1)$. The model is trained to then predict a mixed target $y = \\lambda y_1 + (1-\\lambda) y_2$. This can typically be easily implemented through a weighted loss.\n",
        "\n",
        "ü§î <b><font color='purple'>Question:</font></b> Rewrite the training function to accommodate a mixup procedure. As the model we train is not very big, you can try to only perform mixup a fraction of the time (e.g. 1 time out of 10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxFmouPVsfkS"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Implement Mixup mixing here!\n",
        "            # YOUR WORK\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "\n",
        "             # HERE ALSO\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        running_val_loss = 0.0\n",
        "        for inputs, labels in val_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "              outputs = model(inputs)\n",
        "\n",
        "              # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "              # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)},  Val_Loss: {running_val_loss / len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEhjZwldzCfZ"
      },
      "source": [
        "Since our neural network is not very big, we do not need such strong data augmentation techniques and the stronger augmentation techniques do not bring significant additional benefits compared to normal augmentations. We can still observe some benefit below when only considering mixup (and not classical data augmentation). It is recommended to stick to classical data augmentation in this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsI7_6Wmuhus"
      },
      "outputs": [],
      "source": [
        "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "transforms_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 dataset:\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [40000, 10000], torch.Generator().manual_seed(42))\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)\n",
        "\n",
        "#Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "nn_deep = DeepNN(num_classes=10).to(device)\n",
        "train(nn_deep, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvGP2lWLg0Fg"
      },
      "source": [
        "# Cross-entropy runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqo2i8z4g0Fg"
      },
      "source": [
        "For reproducibility, we need to set the torch manual seed. We train\n",
        "networks using different methods, so to compare them fairly, it makes\n",
        "sense to initialize the networks with the same weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1cN5T-lg0Fh"
      },
      "outputs": [],
      "source": [
        "# Instantiate the lightweight network:\n",
        "torch.manual_seed(42)\n",
        "nn_light = LightNN(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6y7qrhEAP0k"
      },
      "source": [
        "We instantiate one more lightweight network model to compare their\n",
        "performances. Back propagation is sensitive to weight initialization, so\n",
        "we need to make sure these two networks have the exact same\n",
        "initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIg6o4EvAP0k"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "new_nn_light = LightNN(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6AXEkklAP0k"
      },
      "source": [
        "To ensure we have created a copy of the first network, we inspect the\n",
        "norm of its first layer. If it matches, then we are safe to conclude\n",
        "that the networks are indeed the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8PKP-X9AP0k",
        "outputId": "2b87078f-62e5-4c77-fda1-9c5ee64b3bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norm of 1st layer of nn_light: 2.327361822128296\n",
            "Norm of 1st layer of new_nn_light: 2.327361822128296\n"
          ]
        }
      ],
      "source": [
        "# Print the norm of the first layer of the initial lightweight model\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
        "# Print the norm of the first layer of the new lightweight model\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-rUuZ7oAP0l"
      },
      "source": [
        "Print the total number of parameters in each model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGQahmXBAP0l",
        "outputId": "92cd801d-92f4-44d2-97fc-3f4574856be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepNN parameters: 1,186,986\n",
            "LightNN parameters: 267,738\n"
          ]
        }
      ],
      "source": [
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4WgUbcZAP0l"
      },
      "source": [
        "Train and test the lightweight network with cross entropy loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX1xKmbeAP0m"
      },
      "outputs": [],
      "source": [
        "train(nn_light, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_light_ce = test(nn_light, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn4LHGMRAP0m"
      },
      "source": [
        "As we can see, based on test accuracy, we can now compare the deeper\n",
        "network that is to be used as a teacher with the lightweight network\n",
        "that is our supposed student. So far, our student has not intervened\n",
        "with the teacher, therefore this performance is achieved by the student\n",
        "itself. The metrics so far can be seen with the following lines:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdeLhET_AP0m",
        "outputId": "af690529-f075-4e9f-d915-105231ca50f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher accuracy: 80.09%\n",
            "Student accuracy: 69.54%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgKpVA6JAP0n"
      },
      "source": [
        "Logit level knowledge distillation\n",
        "==========================\n",
        "\n",
        "Now let\\'s try to improve the test accuracy of the student network by\n",
        "incorporating the teacher. Knowledge distillation is a straightforward\n",
        "technique to achieve this, based on the fact that both networks output a\n",
        "probability distribution over our classes. Therefore, the two networks\n",
        "share the same number of output neurons. The method works by\n",
        "incorporating an additional loss into the traditional cross entropy\n",
        "loss, which is based on the softmax output of the teacher network. The\n",
        "assumption is that the output activations of a properly trained teacher\n",
        "network carry additional information that can be leveraged by a student\n",
        "network during training. The original work suggests that utilizing\n",
        "ratios of smaller probabilities in the soft targets can help achieve the\n",
        "underlying objective of deep neural networks, which is to create a\n",
        "similarity structure over the data where similar objects are mapped\n",
        "closer together. For example, in CIFAR-10, a truck could be mistaken for\n",
        "an automobile or airplane, if its wheels are present, but it is less\n",
        "likely to be mistaken for a dog. Therefore, it makes sense to assume\n",
        "that valuable information resides not only in the top prediction of a\n",
        "properly trained model but in the entire output distribution. However,\n",
        "cross entropy alone does not sufficiently exploit this information as\n",
        "the activations for non-predicted classes tend to be so small that\n",
        "propagated gradients do not meaningfully change the weights to construct\n",
        "this desirable vector space.\n",
        "\n",
        "As we continue defining our first helper function that introduces a\n",
        "teacher-student dynamic, we need to include a few extra parameters:\n",
        "\n",
        "-   `T`: Temperature controls the smoothness of the output\n",
        "    distributions. Larger `T` leads to smoother distributions, thus\n",
        "    smaller probabilities get a larger boost.\n",
        "-   `soft_target_loss_weight`: A weight assigned to the extra objective\n",
        "    we\\'re about to include.\n",
        "-   `ce_loss_weight`: A weight assigned to cross-entropy. Tuning these\n",
        "    weights pushes the network towards optimizing for either objective.\n",
        "\n",
        "![Distillation loss is calculated from the logits of the networks. It\n",
        "only returns gradients to the\n",
        "student:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/distillation_output_loss.png){.align-center}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6m-s1JUAP0n"
      },
      "outputs": [],
      "source": [
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            #Soften the student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
        "\n",
        "# Compare the student test accuracy with and without the teacher, after distillation\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWWq-pzWAP0n"
      },
      "source": [
        "Feature level knowledge distillation\n",
        "============================\n",
        "\n",
        "Feel free to play around with the temperature parameter that controls\n",
        "the softness of the softmax function and the loss coefficients. In\n",
        "neural networks, it is easy to include additional loss functions to the\n",
        "main objectives to achieve goals like better generalization. Let\\'s try\n",
        "including an objective for the student, but now let\\'s focus on their\n",
        "hidden states rather than their output layers. Our goal is to convey\n",
        "information from the teacher\\'s representation to the student by\n",
        "including a naive loss function, whose minimization implies that the\n",
        "flattened vectors that are subsequently passed to the classifiers have\n",
        "become more *similar* as the loss decreases. Of course, the teacher does\n",
        "not update its weights, so the minimization depends only on the\n",
        "student\\'s weights. The rationale behind this method is that we are\n",
        "operating under the assumption that the teacher model has a better\n",
        "internal representation that is unlikely to be achieved by the student\n",
        "without external intervention, therefore we artificially push the\n",
        "student to mimic the internal representation of the teacher. Whether or\n",
        "not this will end up helping the student is not straightforward, though,\n",
        "because pushing the lightweight network to reach this point could be a\n",
        "good thing, assuming that we have found an internal representation that\n",
        "leads to better test accuracy, but it could also be harmful because the\n",
        "networks have different architectures and the student does not have the\n",
        "same learning capacity as the teacher. In other words, there is no\n",
        "reason for these two vectors, the student\\'s and the teacher\\'s to match\n",
        "per component. The student could reach an internal representation that\n",
        "is a permutation of the teacher\\'s and it would be just as efficient.\n",
        "Nonetheless, we can still run a quick experiment to figure out the\n",
        "impact of this method. We will be using the `CosineEmbeddingLoss` which\n",
        "is given by the following formula:\n",
        "\n",
        "![Formula for\n",
        "CosineEmbeddingLoss](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/cosine_embedding_loss.png){.align-center\n",
        "width=\"450px\"}\n",
        "\n",
        "Obviously, there is one thing that we need to resolve first. When we\n",
        "applied distillation to the output layer we mentioned that both networks\n",
        "have the same number of neurons, equal to the number of classes.\n",
        "However, this is not the case for the layer following our convolutional\n",
        "layers. Here, the teacher has more neurons than the student after the\n",
        "flattening of the final convolutional layer. Our loss function accepts\n",
        "two vectors of equal dimensionality as inputs, therefore we need to\n",
        "somehow match them. We will solve this by including an average pooling\n",
        "layer after the teacher\\'s convolutional layer to reduce its\n",
        "dimensionality to match that of the student.\n",
        "\n",
        "To proceed, we will modify our model classes, or create new ones. Now,\n",
        "the forward function returns not only the logits of the network but also\n",
        "the flattened hidden representation after the convolutional layer. We\n",
        "include the aforementioned pooling for the modified teacher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tsKDbVuAP0o"
      },
      "outputs": [],
      "source": [
        "class ModifiedDeepNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n",
        "        return x, flattened_conv_output_after_pooling\n",
        "\n",
        "# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\n",
        "class ModifiedLightNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        return x, flattened_conv_output\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
        "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Once again ensure the norm of the first layer is the same for both networks\n",
        "print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n",
        "print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
        "\n",
        "# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
        "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1JTxnxiAP0o"
      },
      "source": [
        "Naturally, we need to change the train loop because now the model\n",
        "returns a tuple `(logits, hidden_representation)`. Using a sample input\n",
        "tensor we can print their shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOzwtqsDAP0o"
      },
      "outputs": [],
      "source": [
        "# Create a sample input tensor\n",
        "sample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32\n",
        "\n",
        "# Pass the input through the student\n",
        "logits, hidden_representation = modified_nn_light(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
        "\n",
        "# Pass the input through the teacher\n",
        "logits, hidden_representation = modified_nn_deep(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EhGMtw7AP0p"
      },
      "source": [
        "In our case, `hidden_representation_size` is `1024`. This is the\n",
        "flattened feature map of the final convolutional layer of the student\n",
        "and as you can see, it is the input for its classifier. It is `1024` for\n",
        "the teacher too, because we made it so with `avg_pool1d` from `2048`.\n",
        "The loss applied here only affects the weights of the student prior to\n",
        "the loss calculation. In other words, it does not affect the classifier\n",
        "of the student. The modified training loop is the following:\n",
        "\n",
        "![In Cosine Loss minimization, we want to maximize the cosine similarity\n",
        "of the two representations by returning gradients to the\n",
        "student:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/cosine_loss_distillation.png){.align-center}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EJOSAslAP0s"
      },
      "outputs": [],
      "source": [
        "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    cosine_loss = nn.CosineEmbeddingLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model and keep only the hidden representation\n",
        "            with torch.no_grad():\n",
        "                _, teacher_hidden_representation = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, student_hidden_representation = student(inputs)\n",
        "\n",
        "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
        "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTWmKEtYAP0t"
      },
      "source": [
        "We need to modify our test function for the same reason. Here we ignore\n",
        "the hidden representation returned by the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuOuRxDvAP0t"
      },
      "outputs": [],
      "source": [
        "def test_multiple_outputs(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZpVhOGmAP0t"
      },
      "source": [
        "In this case, we could easily include both knowledge distillation and\n",
        "cosine loss minimization in the same function. It is common to combine\n",
        "methods to achieve better performance in teacher-student paradigms. For\n",
        "now, we can run a simple train-test session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFZxVrvrAP0t"
      },
      "outputs": [],
      "source": [
        "# Train and test the lightweight network with cross entropy loss\n",
        "train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iv3Hf84AP0w"
      },
      "outputs": [],
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
        "print(f\"Student accuracy with CE + CosineLoss: {test_accuracy_light_ce_and_cosine_loss:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVtdTzXSg0Fn"
      },
      "source": [
        "# Improving the teacher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgMYlz8Ig0Fn"
      },
      "source": [
        "As you may have observed, the teacher was not very good. In this part, we propose to improve the teacher and, without modifying the student architecture, to experiment if it also improves the student."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKslV56nePLT"
      },
      "source": [
        "An idea is to start with a pretrained CNN, as we did in the CNN lab. Since the images are very small (32x32x3) compared to the frequent input dimensions (224x224x3) we will use a small pretrained network: EfficientNet b0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kVZhIOiePLT",
        "outputId": "25c53f01-e343-4a0c-9571-21770cc783c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EfficientNet_B0_Weights.IMAGENET1K_V1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/lingrand/.cache/torch/hub/pytorch_vision_main\n"
          ]
        }
      ],
      "source": [
        "weight_enum = torch.hub.load(\"pytorch/vision\", \"get_model_weights\", name=\"efficientnet_b0\")\n",
        "print([weight for weight in weight_enum])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l2gujZRePLU"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "import torchvision.models as tm\n",
        "\n",
        "model = tm.get_model(\"efficientnet_b0\", weights=\"DEFAULT\")\n",
        "weights = EfficientNet_B0_Weights.DEFAULT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poekQMi7ePLU"
      },
      "outputs": [],
      "source": [
        "preprocess = weights.transforms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ-1bOqZePLU",
        "outputId": "543ca210-49f4-430e-f75a-1f78853c5695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[256]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BICUBIC\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Kzw_GpePLW"
      },
      "source": [
        "Let's have a look at the classification part of the original EfficientNet b0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M8gVF_QePLW",
        "outputId": "79072239-2a90-494c-ae79-53c2edb50d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=True)\n",
            "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model.classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdoKabQPePLW"
      },
      "source": [
        "Since we feed the network with tensors of unusual dimensions, we need to modify the `in-features` parameters by a lower value (to be calculated) and modify the number of classes `out_features` by 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHFWGl4jePLW"
      },
      "outputs": [],
      "source": [
        "model.classifier[1] = torch.nn.Linear(in_features=1280, out_features=10, bias=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWDuf7iePLW"
      },
      "source": [
        "Check the new classification part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0g4FucVePLX",
        "outputId": "a62a342b-7cce-4803-8485-433f764dd616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=True)\n",
            "  (1): Linear(in_features=1280, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model.classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDV-vOETePLX"
      },
      "source": [
        "In order to train the classifier first, we will freeze all other parameters of the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PebBhj2yePLX"
      },
      "outputs": [],
      "source": [
        "for param in model.features.parameters():\n",
        "       param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nUrsXmXO_Jb"
      },
      "source": [
        "ü§î <b><font color='purple'>Question:</font></b> Train the classification part only of this network (depending on your GPU, 5 epochs could give you an idea of a model). Be careful that input images are not 224x224. We suggest you to first resize them to 224x224."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbPqaZOIO_Jc"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mpo-E2bO_Jc"
      },
      "source": [
        "ü§î <b><font color='purple'>Question:</font></b> Is it better than the previous teachers? If no, try with augmentations. If yes (directly or after augmentations), learn again the different students. Does it also improve the students?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGtD2pMHO_Jc"
      },
      "outputs": [],
      "source": [
        "# your work"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}